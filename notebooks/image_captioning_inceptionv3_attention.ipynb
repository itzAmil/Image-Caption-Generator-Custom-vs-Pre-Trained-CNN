{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "qN9aYaaINH2v"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1DNR5J8NL_A",
    "outputId": "fbbfc100-42ac-40cc-f8ae-f4530ff0ec09"
   },
   "outputs": [],
   "source": [
    "pip install pydot graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJAttW4UNNC8",
    "outputId": "9d52fd30-d5a0-4dd2-f63b-c2e6b68a24c9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8Y_LB01NsSw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/Project/Image Captioning\" \n",
    "\n",
    "WORKING_DIR = os.path.join(BASE_DIR, \"Inception-v3\")\n",
    "\n",
    "os.chdir(WORKING_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "vhU3iDmx73jf",
    "outputId": "a5563d17-5ce5-4d43-e470-9861a5ee1639"
   },
   "outputs": [],
   "source": [
    "WORKING_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GkmntaQzNH2w",
    "outputId": "40186d8f-c08a-4d33-d50d-971450521068"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "BASE_DIR = \"/kaggle/input/flickr8k\"\n",
    "\n",
    "WORKING_DIR = os.getcwd()\n",
    "\n",
    "print(\"Working Directory:\", WORKING_DIR)\n",
    "print(\"Base Directory:\", BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRl9y3H4NH2x"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow==1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyGX3ChvNH2x",
    "outputId": "100327cf-d6fd-42a9-b1a6-0208afeca31d"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tf.executing_eagerly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYHlCseYNH2y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIhbOCIvNH2y"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Embedding, LSTM, Dropout, Flatten, Input, add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import to_categorical, plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "qwUrdnnbNH2y",
    "outputId": "4ca10075-230f-44cf-a9d3-09e2e034bf46"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "captions_df = pd.read_csv(WORKING_DIR + \"/Data/captions.txt\")\n",
    "\n",
    "captions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLHNP6lbdSVy",
    "outputId": "196c9558-95d0-48c3-d25c-48bc85c3ea67"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "for i in captions_df[\"image\"]:\n",
    "    x = re.search(\"([^\\s]+(\\.(?i)(jpg|png|gif|bmp))$)\", i)\n",
    "    if (x):\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"YES! We have a match!: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7Fy-ZTOd37V",
    "outputId": "6714ef61-b1fb-411a-8061-21cdb2fb4281"
   },
   "outputs": [],
   "source": [
    "captions_df.caption.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6-d0HU9d3hi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcnUYsp0NH2y",
    "outputId": "ff3a8348-6cb9-4bdd-f24c-cd1ee2f0274d"
   },
   "outputs": [],
   "source": [
    "# Importing InceptionV3 model with pre-trained ImageNet weights\n",
    "inception_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "# Getting the input and output layers of the InceptionV3 model\n",
    "new_input = inception_model.input\n",
    "hidden_layer = inception_model.layers[-1].output\n",
    "\n",
    "# Creating a feature extraction model using the input and output layers\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "# Displaying the summary of the feature extraction model\n",
    "image_features_extract_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('/kaggle/working/flickr8k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('/kaggle/working/flickr8k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kSd2CmYNH2z"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the source folder path from the dataset\n",
    "src_folder = '/kaggle/input/'\n",
    "\n",
    "# Specify the destination folder path in your working directory\n",
    "dst_folder = WORKING_DIR + \"/Data\"\n",
    "\n",
    "# Copy the folder and its contents to the working directory\n",
    "shutil.copytree(src_folder, dst_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/kaggle/working/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ky130v4KNH2z",
    "outputId": "e2dbdd2f-3a26-4359-8c21-f68d993a2db7"
   },
   "outputs": [],
   "source": [
    "dir = BASE_DIR + '/Images'\n",
    "\n",
    "img_paths = []\n",
    "\n",
    "for img_name in os.listdir(dir):\n",
    "    img_path = dir + '/' + img_name\n",
    "    img_paths.append(img_path)\n",
    "\n",
    "print(len(img_paths))\n",
    "print(img_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rlg_2yT7NH2z"
   },
   "outputs": [],
   "source": [
    "#from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igXm_Rb-NH2z",
    "outputId": "594835db-77a2-4059-9215-74e1914ed0e5"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image_path_str = tf.strings.as_string(image_path)  # Convert tensor to string tensor\n",
    "    image_name = tf.strings.split(tf.strings.split(image_path_str, '/')[-1], '.')[0]  # Extract image name\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = preprocess_input(img)\n",
    "    return img, image_name\n",
    "\n",
    "img = load_image(BASE_DIR + '/Images/3582465732_78f77f34ae.jpg')[0]\n",
    "print(img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thYYYHm6NH2z",
    "outputId": "1aed5e5f-d615-4fc8-8f9f-904e54a051bb"
   },
   "outputs": [],
   "source": [
    "image_dataset = tf.data.Dataset.from_tensor_slices(img_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)\n",
    "\n",
    "print(f\"Let's check the BatchDataset shapes--> {image_dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pMSYadKNH20",
    "outputId": "d7b1d1d6-329c-47f3-be63-4b812276a6a8"
   },
   "outputs": [],
   "source": [
    "image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsJL89Z0NH20"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for img, path in tqdm(image_dataset):\n",
    "#   batch_features = image_features_extract_model(img)\n",
    "#   batch_features = tf.reshape(batch_features,\n",
    "#                               (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "#   for bf, p in zip(batch_features, path):\n",
    "#     path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "#     np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvHJHNjtNH20"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "save_dir = 'kaggle/working/Features'\n",
    "\n",
    "#shutil.rmtree('/kaggle/working/flickr8k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnS5WcnDNH20"
   },
   "outputs": [],
   "source": [
    " os.makedirs('Inception_Features', exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-gVQOpRNH20"
   },
   "outputs": [],
   "source": [
    "os.remove('/kaggle/working/Features/Tensor(\"args_0:0\", shape=(), dtype=string).pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwfGqX6LNH21"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree('/content/drive/MyDrive/Project/Image Captioning/checkpoint_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i57cw6wONH21"
   },
   "outputs": [],
   "source": [
    "# for item in image_dataset.take(1):\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "396d12f8a7ea46ddaceb2dedb045588a",
      "6128f98989e444ba987174a6b69c0d52",
      "7733197a809746c490ec4d1f78f7ac88",
      "2dc68e4a4f1d4ed68ed7f72cfad00858",
      "f43c9a7709584e91aae13fa95f8c36e4",
      "c638bb59e4a742c1959220562fff1e38",
      "484fa27c39a042bb845ac3b1f41b716d",
      "0df0d0d3dd5843d89cbd2f7fffc2baba",
      "e66b9008c39344769a97f29d4a959a32",
      "33c91f452eec40c0ac490fa31aead431",
      "5e9b54f633d54bd5a9fb58d93752fe38"
     ]
    },
    "id": "OorwOR5KNH21",
    "outputId": "4a97e7e0-7da1-4aa0-b5fe-da05e18c194b"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_dir = tf.constant(WORKING_DIR + '/Inception_Features')\n",
    "save_dir = save_dir.numpy().decode('utf-8')\n",
    "\n",
    "for img, name in tqdm(image_dataset):\n",
    "  batch_features = image_features_extract_model(img)\n",
    "  batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "  for bf, image_name in zip(batch_features, name):\n",
    "    image_feature_name = image_name.numpy().decode('utf-8')\n",
    "    #image_feature_name = p.numpy().decode('utf-8')\n",
    "    #full_path = os.path.join(save_dir, image_feature_name)\n",
    "    #print(save_dir)\n",
    "    #print(image_feature_name)\n",
    "\n",
    "    with open(os.path.join(save_dir, image_feature_name + '.pkl'),'wb') as f:\n",
    "        pickle.dump(bf,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lHla34rNH21",
    "outputId": "087cdcd5-6c67-4045-fe89-3b19ce8a3b32"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "save_dir = WORKING_DIR + '/Inception_Features'\n",
    "\n",
    "for i in os.listdir(save_dir):\n",
    "    count += 1\n",
    "\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngpk_s0tNH21"
   },
   "outputs": [],
   "source": [
    "tf.data.Dataset.from_tensor_slices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ur_iGxmINH21",
    "outputId": "e7135840-c625-43ac-9c8b-3019c69d1e15"
   },
   "outputs": [],
   "source": [
    "bf.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "pk-2WCNINH21",
    "outputId": "117bcd7b-dbf1-4f19-9af9-39f991497af7"
   },
   "outputs": [],
   "source": [
    "batch_features.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7FBm1s6NH21",
    "outputId": "b7b7756d-3c7f-415f-f3a9-f9e4530b5936"
   },
   "outputs": [],
   "source": [
    "len(captions_df.caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyNWPkKwNH22",
    "outputId": "2088b813-4109-4fc8-bc25-96540a1a19ad"
   },
   "outputs": [],
   "source": [
    "len(set(captions_df.image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXMRe0V1NH22",
    "outputId": "14880025-3179-4ef4-e2ed-368b510e385b"
   },
   "outputs": [],
   "source": [
    "def get_name(path):\n",
    "    name = os.path.basename(path)\n",
    "    name = name.split('.')[0]\n",
    "    return name\n",
    "\n",
    "name = get_name(BASE_DIR + '/Images/2567035103_3511020c8f.jpg')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmggD-cGNH22",
    "outputId": "71dbf0f2-56ab-45d7-f48e-92154347c46b"
   },
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "\n",
    "for caption in captions_df['caption']:\n",
    "    caption = \"<sos> \" + caption + \" <eos>\"\n",
    "    all_captions.append(caption)\n",
    "\n",
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e63CUwUkNH22",
    "outputId": "02ec5334-e338-405f-843c-7d64e869cf06"
   },
   "outputs": [],
   "source": [
    "image_names = []\n",
    "\n",
    "for name in captions_df['image']:\n",
    "    image_name = get_name(name)\n",
    "    image_names.append(image_name)\n",
    "\n",
    "image_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wpNZfdoNH22"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "shuffled_captions, shuffled_image_names = shuffle(all_captions,\n",
    "                                                  image_names,\n",
    "                                                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSrTRe_m938H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFoodXmlNH22"
   },
   "outputs": [],
   "source": [
    "#len(shuffled_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjG5aEZ-NH23"
   },
   "outputs": [],
   "source": [
    "# len(shuffled_image_names)\n",
    "# shuffled_image_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zTntA18NH23"
   },
   "outputs": [],
   "source": [
    "#function for getting maxlen of the caption\n",
    "\n",
    "def get_maxlen(captions):\n",
    "    len_caption = []\n",
    "    for caption in captions_df['caption']:\n",
    "        len_caption.append(len(caption.split()))\n",
    "\n",
    "    return max(len_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFodJsNyNH23"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3f7VOlpNH23",
    "outputId": "2f93fad3-f244-490b-b9e2-84cc5ecfd18b"
   },
   "outputs": [],
   "source": [
    "# utilizing the tokenizer\n",
    "\n",
    "top_k = 5000\n",
    "tokenizer = Tokenizer(num_words = top_k, oov_token = '<unk>', filters = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "\n",
    "tokenizer.fit_on_texts(shuffled_captions)\n",
    "\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "train_seqs = tokenizer.texts_to_sequences(shuffled_captions)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tbmgn26kNH23",
    "outputId": "a1a4666e-53e6-4963-8073-e0fdfda63af2"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "8UtShvGZat-h",
    "outputId": "82516f7e-bd64-47eb-8be1-1313f3f40108"
   },
   "outputs": [],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOxfIH_xZftI"
   },
   "outputs": [],
   "source": [
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_GBgaFsNH23",
    "outputId": "aea090e9-0b02-481c-a6e2-59f2faff18bd"
   },
   "outputs": [],
   "source": [
    "max_length = get_maxlen(train_seqs)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRLyXGyWNH24",
    "outputId": "d3a1a69d-e56e-485d-e824-3b9484650eb0"
   },
   "outputs": [],
   "source": [
    "caption_vector = pad_sequences(train_seqs, maxlen = max_length, padding = 'post')\n",
    "print(caption_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DO4zEBM7NH2-"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_image, test_image, train_caption, test_caption = train_test_split(shuffled_image_names,\n",
    "                                                                        caption_vector,\n",
    "                                                                       test_size = 0.2,\n",
    "                                                                       random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ur728JjBNH2_",
    "outputId": "7a6f5623-892c-4564-e861-c8bb89a436fc"
   },
   "outputs": [],
   "source": [
    "len(train_image), len(test_image), len(train_caption), len(test_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "9NYEwj4bNH2_",
    "outputId": "b491ffee-e622-4856-8303-d2655f885804"
   },
   "outputs": [],
   "source": [
    "train_image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "an0MJrOANH2_",
    "outputId": "a0447456-9c72-4570-e20d-150f8c0adf1c"
   },
   "outputs": [],
   "source": [
    "train_caption[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtUqd38qNH2_"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "num_steps = len(train_image) // BATCH_SIZE\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWtfoW84NH2_",
    "outputId": "db3aa1ef-acd9-464c-fec8-b4ff09c2ee4f"
   },
   "outputs": [],
   "source": [
    "\n",
    "save_dir = tf.constant(WORKING_DIR + '/Inception_Features')\n",
    "save_dir = save_dir.numpy().decode('utf-8')\n",
    "\n",
    "def map_func(image_name, caption):\n",
    "    name = tf.constant(image_name)\n",
    "    image_name_str = name.numpy().decode('utf-8')\n",
    "    with open(os.path.join(str(save_dir), image_name_str + \".pkl\"),'rb') as f:\n",
    "        img_tensor = pickle.load(f)\n",
    "    return img_tensor, caption\n",
    "\n",
    "tensor, cap = map_func(train_image[0], train_caption[0])\n",
    "print(tensor)\n",
    "print(cap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcoggHLiNH2_"
   },
   "outputs": [],
   "source": [
    "# for i in os.listdir(features_dir):\n",
    "#     name = i.split('.')[0]\n",
    "#     if name == train_image[0]:\n",
    "#         print(\"found\")\n",
    "#         break\n",
    "#     else:\n",
    "#         continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IO3ON2sDNH3A"
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_image, train_caption))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqJ-T0hDNH3A",
    "outputId": "ce6625d7-05a9-4b4e-8680-0c7173b828fa"
   },
   "outputs": [],
   "source": [
    "for item in dataset.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSPbmh-INH3A",
    "outputId": "ef01ae1d-cbe1-4c8c-801c-d56dfa3c7306"
   },
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FfjNP4xNH3A"
   },
   "outputs": [],
   "source": [
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtYjkO6iNH3A"
   },
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSIxQiChNH3B"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # score shape == (batch_size, 64, hidden_size)\n",
    "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    # you get 1 at the last axis because you are applying score to self.V\n",
    "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = tf.keras.layers.LSTM(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state, c_state = self.lstm(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2XjqZA4xl4U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-r77mZ_gtvYh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dheymkaNH3B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQDjFPBkNH3B"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6qWHtPMNH3B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8Y-xlN0NH3B"
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 64\n",
    "# BUFFER_SIZE = 1000\n",
    "# embedding_dim = 256\n",
    "# units = 512\n",
    "# num_steps = len(train_image) // BATCH_SIZE\n",
    "# features_shape = 2048\n",
    "# attention_features_shape = 64\n",
    "\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWNRN0TiNH3C"
   },
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "id": "MDtY_VPt1NvG",
    "outputId": "e76c509a-9764-4ebe-aa76-63b37f95c60d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Plot the CNN Encoder model\n",
    "tf.keras.utils.plot_model(encoder, to_file='encoder_model.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "id": "xX0ZGzn01OR6",
    "outputId": "04989289-a672-49bc-8848-9b8e7bc783e8"
   },
   "outputs": [],
   "source": [
    "# Plot the RNN Decoder model\n",
    "\n",
    "tf.keras.utils.plot_model(decoder, to_file='decoder_model.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ACBM5e0NH3C"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRBPyw8yNH3C"
   },
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iwh1E3vhNH3C"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Df3iFHnfNH3C"
   },
   "outputs": [],
   "source": [
    "# saving checkpoint\n",
    "\n",
    "checkpoint_path_ckpt = WORKING_DIR + \"/checkpoint_final/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path_ckpt, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1T5uYnGNNH3C"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYn7nd0hNH3D"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0jMIarCW2cw"
   },
   "outputs": [],
   "source": [
    "# Function to save the entire model\n",
    "def save_model(encoder, decoder):\n",
    "    tf.saved_model.save(encoder, ckpt_manager.save_checkpoint_path)\n",
    "    tf.saved_model.save(encoder, ckpt_manager.save_checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "0cXSWrAKNH3D",
    "outputId": "bbb3dc66-24d5-4449-8222-dd4556399c28"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "loss_plot = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<sos>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "        total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        return loss, total_loss\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "        \n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "loss_plot = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<sos>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "        total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        return loss, total_loss\n",
    "\n",
    "\n",
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "        \n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORKING_DIR + '/loss_plot.pkl','wb') as f:\n",
    "    pickle.dump(loss_plot, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bek6cWsUNH3D"
   },
   "outputs": [],
   "source": [
    "checkpoint_path_ckpt = WORKING_DIR + \"/checkpoint_final/train\"\n",
    "\n",
    "checkpoint_path = checkpoint_path_ckpt\n",
    "#tf.compat.v2.keras.callbacks.ModelCheckpoint\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                               decoder=decoder,\n",
    "                               optimizer = optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path_ckpt, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    # Restore the latest checkpoint\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORKING_DIR + '/loss_plot.pkl', 'rb') as f:\n",
    "    loss_plot = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "HCJZKY68NH3D",
    "outputId": "fdf50c01-91d2-4154-a80c-2b4ab9c64775"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iy7G1IyONH3D"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "#     img_path = tf.constant(BASE_DIR + '/Images/' + image)\n",
    "#     print(img_path)\n",
    "#     img = load_image(img_path)[0]\n",
    "\n",
    "#     temp_input = tf.expand_dims(img, 0)\n",
    "#     img_tensor_val = image_features_extract_model(temp_input)\n",
    "\n",
    "    name = image.split('.')[0]\n",
    "\n",
    "    with open(WORKING_DIR + '/Inception_Features/' + name + \".pkl\",\"rb\") as t:\n",
    "        img_tensor_val = pickle.load(t)\n",
    "        \n",
    "    #img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<sos>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<eos>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fy4uHH8RNH3E"
   },
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(os.path.join(BASE_DIR, 'Images',image)))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbS8zz0LNH3E"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def predict_caption(image):\n",
    "  real_caption = ' '.join([tokenizer.index_word[i] for i in test_caption[rid] if i not in [0]])\n",
    "  result, attention_plot = evaluate(image)\n",
    "\n",
    "  for i in result:\n",
    "      if i==\"<unk>\":\n",
    "          result.remove(i)\n",
    "\n",
    "  print ('Real Caption:', real_caption)\n",
    "  print ('Prediction Caption:', ' '.join(result))\n",
    "  plot_attention(image, result, attention_plot)\n",
    "  # opening the image\n",
    "  #Image.open(os.path.join(WORKING_DIR, 'Images', image + \".jpg\"))\n",
    "  path = os.path.join(BASE_DIR, 'Images', image)\n",
    "  img = mpimg.imread(path)\n",
    "  plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "id": "IuPbwB6ptodU",
    "outputId": "53620e7e-4c66-45d8-ff06-24650d524b01"
   },
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "id": "kCOpPfPMuM3o",
    "outputId": "b171d381-1a2b-4550-a9e4-d3008dbebcfa"
   },
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 766
    },
    "id": "wjUHd-5rwxQu",
    "outputId": "f8f83302-a12d-4215-d7c5-3514e70c2ec7"
   },
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VcG-0f0Sw44P",
    "outputId": "81dd7b84-8bf1-4d5e-aec2-6ce842273089"
   },
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9vFZuxLxt9N"
   },
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "to926OucynDN",
    "outputId": "4ab902cb-236f-4c10-ed96-abf9d2ac7070"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'a', 'test','of', 'cricket', 'match']]\n",
    "candidate = ['this', 'is', 'a', 'test','of', 'cricket', 'match']\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "id": "VoRMz6FIyxyV",
    "outputId": "c5275109-8ac6-4621-e27a-ffbead92734f"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "start = time.time()\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in test_caption[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "\n",
    "#remove \"<unk>\" in result\n",
    "for i in result:\n",
    "    if i==\"<unk>\":\n",
    "        result.remove(i)\n",
    "\n",
    "\n",
    "#remove <end> from result\n",
    "result_join = ' '.join(result)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = result_final\n",
    "score = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
    "print(f\"BLEU score: {score}\")\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', result_final)\n",
    "plot_attention(image, result, attention_plot)\n",
    "\n",
    "print(f\"time took to Predict: {round(time.time()-start)} sec\")\n",
    "# opening the image\n",
    "path = os.path.join(BASE_DIR, 'Images', image)\n",
    "img = mpimg.imread(path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AtAvDRQGz6dW",
    "outputId": "5297a190-837d-409e-e32e-decd1a47302e"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "start = time.time()\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in test_caption[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "\n",
    "#remove \"<unk>\" in result\n",
    "for i in result:\n",
    "    if i==\"<unk>\":\n",
    "        result.remove(i)\n",
    "\n",
    "\n",
    "#remove <end> from result\n",
    "result_join = ' '.join(result)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = result_final\n",
    "score = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
    "print(f\"BLEU score: {score*100}\")\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', result_final)\n",
    "plot_attention(image, result, attention_plot)\n",
    "\n",
    "print(f\"time took to Predict: {round(time.time()-start)} sec\")\n",
    "# opening the image\n",
    "path = os.path.join(BASE_DIR, 'Images', image)\n",
    "img = mpimg.imread(path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "id": "WRHc8jvg0S1u",
    "outputId": "52f8df99-4c3e-4ce7-f064-8fd404141c79"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "start = time.time()\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in test_caption[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "\n",
    "#remove \"<unk>\" in result\n",
    "for i in result:\n",
    "    if i==\"<unk>\":\n",
    "        result.remove(i)\n",
    "\n",
    "\n",
    "#remove <end> from result\n",
    "result_join = ' '.join(result)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = result_final\n",
    "score = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
    "print(f\"BLEU score: {score*100}\")\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', result_final)\n",
    "plot_attention(image, result, attention_plot)\n",
    "\n",
    "print(f\"time took to Predict: {round(time.time()-start)} sec\")\n",
    "# opening the image\n",
    "path = os.path.join(BASE_DIR, 'Images', image)\n",
    "img = mpimg.imread(path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jR_Ey_aINH3E",
    "outputId": "d687ed93-135f-4a68-c232-159bb6290617"
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'file.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2BceFpdNH3E"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "def zip_dir(directory = os.path.join(WORKING_DIR), file_name = 'Features.zip'):\n",
    "    \"\"\"\n",
    "    zip all the files in a directory\n",
    "\n",
    "    Parameters\n",
    "    _____\n",
    "    directory: str\n",
    "        directory needs to be zipped, defualt is current working directory\n",
    "\n",
    "    file_name: str\n",
    "        the name of the zipped file (including .zip), default is 'directory.zip'\n",
    "\n",
    "    Returns\n",
    "    _____\n",
    "    Creates a hyperlink, which can be used to download the zip file)\n",
    "    \"\"\"\n",
    "    os.chdir(directory)\n",
    "    zip_ref = zipfile.ZipFile(file_name, mode='w')\n",
    "    for folder, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file_name in file:\n",
    "                pass\n",
    "            else:\n",
    "                zip_ref.write(os.path.join(folder, file))\n",
    "\n",
    "    return FileLink(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nex6IWmNH3E",
    "outputId": "b65b35b3-d3a3-4ba6-938e-942274a44102"
   },
   "outputs": [],
   "source": [
    "#!zip -r file.zip /kaggle/working/Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qANHWdkNH3E"
   },
   "outputs": [],
   "source": [
    "# reading captions\n",
    "import os \n",
    "\n",
    "with open(os.path.join(BASE_DIR,\"captions.txt\"),\"r\") as c:\n",
    "  next(c)\n",
    "  read_captions = c.read() # reading the file 'captions.txt' into read_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating mapping of image to their captions\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def create_mapping(doc):\n",
    "  mapping = {}\n",
    "\n",
    "  for line in tqdm(doc.split(\"\\n\")):\n",
    "    if len(line) < 2:\n",
    "      continue\n",
    "    words = line.split(\",\")\n",
    "    img_name = words[0]\n",
    "    image_id = img_name.split(\".\")[0]\n",
    "\n",
    "    caption = words[1:]\n",
    "    caption = \" \".join(caption)\n",
    "\n",
    "    if image_id not in mapping:\n",
    "      mapping[image_id] = []\n",
    "\n",
    "    mapping[image_id].append(caption)\n",
    "\n",
    "  return mapping\n",
    "\n",
    "mapping = create_mapping(read_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(WORKING_DIR,'mapping.pkl'),'wb') as f:\n",
    "    pickle.dump(mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping['3329793486_afc16663cc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_ids = list(mapping.keys())\n",
    "\n",
    "train, test = train_test_split(image_ids, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(WORKING_DIR,'mapping.pkl'),'rb') as r:\n",
    "    mapping = pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(mapping):\n",
    "  for id,captions in mapping.items():\n",
    "    for i in range(len(captions)):\n",
    "      caption = captions[i]\n",
    "      \n",
    "      temp = []\n",
    "      for word in caption.split():\n",
    "        if len(word) > 1:\n",
    "          temp.append(word)\n",
    "      caption = \"<SOS> \" + ' '.join(temp) + \" <EOS>\"\n",
    "      captions[i] = caption\n",
    "\n",
    "preprocess(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping['3329793486_afc16663cc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    captions = mapping[key]\n",
    "    #print(captions)\n",
    "    \n",
    "    key = key + \".jpg\"\n",
    "    result, _ = evaluate(key)\n",
    "    \n",
    "    for i in result:\n",
    "      if i==\"<unk>\":\n",
    "          result.remove(i)\n",
    "    \n",
    "    actual_captions = []\n",
    "    \n",
    "        \n",
    "    for caption in captions:\n",
    "        cap = list()\n",
    "        words = caption.split()\n",
    "        \n",
    "        for word in words:\n",
    "            if word in ['<SOS>','<EOS>']:\n",
    "                continue\n",
    "            cap.append(word)\n",
    "        actual_captions.append(cap)\n",
    "    \n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(result)\n",
    "    \n",
    "#     print(actual)\n",
    "#     print(predicted)\n",
    "   \n",
    "\n",
    "print(\"Bleu Score for Image Captioning(Inceptionv3)\")\n",
    "\n",
    "smoothing_function = SmoothingFunction()\n",
    "\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0), smoothing_function = smoothing_function.method1))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0), smoothing_function = smoothing_function.method1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORKING_DIR + '/actual.pkl','wb') as a:\n",
    "    pickle.dump(actual, a)\n",
    "    \n",
    "with open(WORKING_DIR + '/predicted.pkl','wb') as p:\n",
    "    pickle.dump(predicted, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(WORKING_DIR + '/actual.pkl','rb') as f:\n",
    "    actual = pickle.load(f)\n",
    "    \n",
    "with open(WORKING_DIR + '/predicted.pkl','rb') as r:\n",
    "    predicted = pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "smoothing_function = SmoothingFunction()\n",
    "\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0), smoothing_function = smoothing_function.method1))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0), smoothing_function = smoothing_function.method1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyciderevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import cider\n",
    "\n",
    "cider_scorer = cider.Cider()\n",
    "\n",
    "# Compute CIDEr score\n",
    "cider_score, _ = cider_scorer.compute_score(actual, predicted)\n",
    "\n",
    "print(\"CIDEr score:\", cider_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual.pop()\n",
    "predicted.pop()\n",
    "\n",
    "len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "sent_actual, sent_predicted = list(), list()\n",
    "\n",
    "for key in tqdm(['2642350864_099c0f2152']):\n",
    "    captions = mapping[key]\n",
    "    #print(captions)\n",
    "    \n",
    "    key = key + \".jpg\"\n",
    "    result, _ = evaluate(key)\n",
    "    \n",
    "    for i in result:\n",
    "      if i==\"<unk>\":\n",
    "          result.remove(i)\n",
    "            \n",
    "    #result = ' '.join(result)\n",
    "    \n",
    "    #actual_captions = [caption.split() for caption in captions]\n",
    "    \n",
    "    all_caps = []\n",
    "    \n",
    "    for caption in captions:\n",
    "        cap = list()\n",
    "        words = caption.split()\n",
    "        for word in words:\n",
    "            if word in ['<SOS>','<EOS>']:\n",
    "                continue\n",
    "            cap.append(word)\n",
    "        all_caps.append(cap)\n",
    "            \n",
    "    \n",
    "    sent_actual.append(all_caps)\n",
    "    sent_predicted.append(result)\n",
    "    \n",
    "    print(sent_actual)\n",
    "    print(sent_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BLEU-1: %f\" % corpus_bleu(sent_actual, sent_predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(sent_actual, sent_predicted, weights=(0.5, 0.5, 0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "score = meteor_score(actual, predicted)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sentence_bleu(sent_actual, sent_predicted, weights=(1.0, 0.0, 0, 0))\n",
    "print(f\"BLEU score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
