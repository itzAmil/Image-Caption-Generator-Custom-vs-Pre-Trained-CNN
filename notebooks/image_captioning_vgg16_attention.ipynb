{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "BASE_DIR = \"/kaggle/input/flickr8k\"\n",
    "\n",
    "WORKING_DIR = os.getcwd()\n",
    "\n",
    "print(\"Working Directory:\", WORKING_DIR)\n",
    "print(\"Base Directory:\", BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow==1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tf.executing_eagerly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Embedding, LSTM, Dropout, Flatten, Input, add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import to_categorical, plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "captions_df = pd.read_csv(os.path.join(BASE_DIR, 'captions.txt'))\n",
    "\n",
    "captions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.VGG16(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "image_features_extract_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the source folder path from the dataset\n",
    "src_folder = '/kaggle/input/flickr8k'\n",
    "\n",
    "# Specify the destination folder path in your working directory\n",
    "dst_folder = WORKING_DIR + \"/flickr8k\"\n",
    "\n",
    "# Copy the folder and its contents to the working directory\n",
    "shutil.copytree(src_folder, dst_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = WORKING_DIR + '/flickr8k/Images'\n",
    "\n",
    "img_paths = []\n",
    "\n",
    "for img_name in os.listdir(dir):\n",
    "    img_path = dir + '/' + img_name\n",
    "    img_paths.append(img_path)\n",
    "    \n",
    "print(len(img_paths))\n",
    "print(img_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image_path_str = tf.strings.as_string(image_path)  # Convert tensor to string tensor\n",
    "    image_name = tf.strings.split(tf.strings.split(image_path_str, '/')[-1], '.')[0]  # Extract image name\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (224, 224))\n",
    "    img = preprocess_input(img)\n",
    "    return img, image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = tf.data.Dataset.from_tensor_slices(img_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)\n",
    "\n",
    "print(f\"Let's check the BatchDataset shapes--> {image_dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for img, path in tqdm(image_dataset):\n",
    "#   batch_features = image_features_extract_model(img)\n",
    "#   batch_features = tf.reshape(batch_features,\n",
    "#                               (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "#   for bf, p in zip(batch_features, path):\n",
    "#     path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "#     np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "save_dir = 'kaggle/working/Features'\n",
    "\n",
    "#shutil.rmtree('/kaggle/working/flickr8k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " os.makedirs('Features', exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('/kaggle/working/Features/Tensor(\"args_0:0\", shape=(), dtype=string).pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('/kaggle/working/kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in image_dataset.take(1):\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_dir = tf.constant(WORKING_DIR + '/Features')\n",
    "save_dir = save_dir.numpy().decode('utf-8')\n",
    "\n",
    "for img, name in tqdm(image_dataset):\n",
    "  batch_features = image_features_extract_model(img)\n",
    "  batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "  for bf, image_name in zip(batch_features, name):\n",
    "    image_feature_name = image_name.numpy().decode('utf-8')\n",
    "    #image_feature_name = p.numpy().decode('utf-8')\n",
    "    #full_path = os.path.join(save_dir, image_feature_name)\n",
    "    #print(save_dir)\n",
    "    #print(image_feature_name)\n",
    "   \n",
    "    with open(os.path.join(save_dir, image_feature_name + '.pkl'),'wb') as f:\n",
    "        pickle.dump(bf,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "save_dir = '/kaggle/working/Features'\n",
    "\n",
    "for i in os.listdir(save_dir):\n",
    "    count += 1\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.Dataset.from_tensor_slices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_features.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(captions_df.caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(captions_df.image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(path):\n",
    "    name = os.path.basename(path)\n",
    "    name = name.split('.')[0]\n",
    "    return name\n",
    "\n",
    "name = get_name('/kaggle/working/flickr8k/Images/2567035103_3511020c8f.jpg')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "\n",
    "for caption in captions_df['caption']:\n",
    "    caption = \"<SOS> \" + caption + \" <EOS>\"\n",
    "    all_captions.append(caption)\n",
    "    \n",
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = []\n",
    "\n",
    "for name in captions_df['image']:\n",
    "    image_name = get_name(name)\n",
    "    image_names.append(image_name)\n",
    "    \n",
    "image_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "shuffled_captions, shuffled_image_names = shuffle(all_captions,\n",
    "                                                  image_names,\n",
    "                                                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(shuffled_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(shuffled_image_names)\n",
    "# shuffled_image_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for getting maxlen of the caption\n",
    "\n",
    "def get_maxlen(captions):\n",
    "    len_caption = []\n",
    "    for caption in captions_df['caption']:\n",
    "        len_caption.append(len(caption.split()))\n",
    "\n",
    "    return max(len_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizing the tokenizer \n",
    "\n",
    "tokenizer = Tokenizer(oov_token = '<unk>', filters = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "\n",
    "tokenizer.fit_on_texts(shuffled_captions)\n",
    "\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "train_seqs = tokenizer.texts_to_sequences(shuffled_captions)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<sos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = get_maxlen(train_seqs)\n",
    "print(max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_vector = pad_sequences(train_seqs, maxlen = max_length, padding = 'post')\n",
    "print(caption_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_image, test_image, train_caption, test_caption = train_test_split(shuffled_image_names,\n",
    "                                                                        caption_vector,\n",
    "                                                                       test_size = 0.2,\n",
    "                                                                       random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_image), len(test_image), len(train_caption), len(test_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_caption[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "num_steps = len(train_image) // BATCH_SIZE\n",
    "features_shape = 512\n",
    "attention_features_shape = 49\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dir = tf.constant(WORKING_DIR + '/Features')\n",
    "save_dir = save_dir.numpy().decode('utf-8')\n",
    "\n",
    "def map_func(image_name, caption):\n",
    "    name = tf.constant(image_name)\n",
    "    image_name_str = name.numpy().decode('utf-8')\n",
    "    with open(os.path.join(str(save_dir), image_name_str + \".pkl\"),'rb') as f:\n",
    "        img_tensor = pickle.load(f)\n",
    "    return img_tensor, caption\n",
    "\n",
    "tensor, cap = map_func(train_image[0], train_caption[0])\n",
    "print(tensor)\n",
    "print(cap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in os.listdir(features_dir):\n",
    "#     name = i.split('.')[0]\n",
    "#     if name == train_image[0]:\n",
    "#         print(\"found\")\n",
    "#         break\n",
    "#     else:\n",
    "#         continue\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_image, train_caption))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # score shape == (batch_size, 64, hidden_size)\n",
    "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    # you get 1 at the last axis because you are applying score to self.V\n",
    "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = tf.keras.layers.LSTM(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state, c_state = self.lstm(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 64\n",
    "# BUFFER_SIZE = 1000\n",
    "# embedding_dim = 256\n",
    "# units = 512\n",
    "# num_steps = len(train_image) // BATCH_SIZE\n",
    "# features_shape = 512\n",
    "# attention_features_shape = 49\n",
    "\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving checkpoint\n",
    "\n",
    "checkpoint_path_ckpt = WORKING_DIR + \"/checkpoint_final/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path_ckpt, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "loss_plot = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<sos>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "        total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        return loss, total_loss\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "loss_plot = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<sos>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "        total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        return loss, total_loss\n",
    "\n",
    "\n",
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_losss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_ckpt = WORKING_DIR + \"/checkpoint_final/train\"\n",
    "\n",
    "checkpoint_path = checkpoint_path_ckpt\n",
    "#tf.compat.v2.keras.callbacks.ModelCheckpoint\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                               decoder=decoder,\n",
    "                               optimizer = optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path_ckpt, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    # Restore the latest checkpoint\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORKING_DIR + \"/loss_plot\", 'wb') as f:\n",
    "    pickle.dump(loss_plot, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORKING_DIR + \"/loss_plot\", \"rb\") as f:\n",
    "    loss_plot = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "#     img_path = tf.constant(BASE_DIR + '/Images/' + image)\n",
    "#     print(img_path)\n",
    "#     img = load_image(img_path)[0]\n",
    "\n",
    "#     temp_input = tf.expand_dims(img, 0)\n",
    "#     img_tensor_val = image_features_extract_model(temp_input)\n",
    "\n",
    "    name = image.split('.')[0]\n",
    "\n",
    "    with open(WORKING_DIR + '/Features/' + name + \".pkl\",\"rb\") as t:\n",
    "        img_tensor_val = pickle.load(t)\n",
    "        \n",
    "    #img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<sos>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<eos>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(os.path.join(WORKING_DIR, 'flickr8k/Images',image)))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def predict_caption(image):\n",
    "  real_caption = ' '.join([tokenizer.index_word[i] for i in test_caption[rid] if i not in [0]])\n",
    "  result, attention_plot = evaluate(image)\n",
    "\n",
    "  for i in result:\n",
    "      if i==\"<unk>\":\n",
    "          result.remove(i)\n",
    "\n",
    "  print ('Real Caption:', real_caption)\n",
    "  print ('Prediction Caption:', ' '.join(result))\n",
    "  plot_attention(image, result, attention_plot)\n",
    "  # opening the image\n",
    "  #Image.open(os.path.join(WORKING_DIR, 'Images', image + \".jpg\"))\n",
    "  path = os.path.join(WORKING_DIR, 'flickr8k/Images', image)\n",
    "  img = mpimg.imread(path)\n",
    "  plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "predict_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "start = time.time()\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in test_caption[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "\n",
    "#remove \"<unk>\" in result\n",
    "for i in result:\n",
    "    if i==\"<unk>\":\n",
    "        result.remove(i)\n",
    "\n",
    "\n",
    "#remove <end> from result\n",
    "result_join = ' '.join(result)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = result_final\n",
    "score = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
    "print(f\"BLEU score: {score}\")\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', result_final)\n",
    "plot_attention(image, result, attention_plot)\n",
    "\n",
    "print(f\"time took to Predict: {round(time.time()-start)} sec\")\n",
    "# opening the image\n",
    "path = os.path.join(BASE_DIR, 'flickr8k/Images', image)\n",
    "img = mpimg.imread(path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "rid = np.random.randint(0, len(test_image))\n",
    "image = test_image[rid] + '.jpg'\n",
    "\n",
    "start = time.time()\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in test_caption[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "\n",
    "#remove \"<unk>\" in result\n",
    "for i in result:\n",
    "    if i==\"<unk>\":\n",
    "        result.remove(i)\n",
    "\n",
    "\n",
    "#remove <end> from result\n",
    "result_join = ' '.join(result)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = result_final\n",
    "score = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
    "print(f\"BLEU score: {score}\")\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', result_final)\n",
    "plot_attention(image, result, attention_plot)\n",
    "\n",
    "print(f\"time took to Predict: {round(time.time()-start)} sec\")\n",
    "# opening the image\n",
    "path = os.path.join(BASE_DIR, 'flickr8k/Images', image)\n",
    "img = mpimg.imread(path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'file.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import os\n",
    "# from IPython.display import FileLink\n",
    "\n",
    "# def zip_dir(directory = os.path.join(WORKING_DIR), file_name = 'Features.zip'):\n",
    "#     \"\"\"\n",
    "#     zip all the files in a directory\n",
    "    \n",
    "#     Parameters\n",
    "#     _____\n",
    "#     directory: str\n",
    "#         directory needs to be zipped, defualt is current working directory\n",
    "        \n",
    "#     file_name: str\n",
    "#         the name of the zipped file (including .zip), default is 'directory.zip'\n",
    "        \n",
    "#     Returns\n",
    "#     _____\n",
    "#     Creates a hyperlink, which can be used to download the zip file)\n",
    "#     \"\"\"\n",
    "#     os.chdir(directory)\n",
    "#     zip_ref = zipfile.ZipFile(file_name, mode='w')\n",
    "#     for folder, _, files in os.walk(directory):\n",
    "#         for file in files:\n",
    "#             if file_name in file:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 zip_ref.write(os.path.join(folder, file))\n",
    "\n",
    "#     return FileLink(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!zip -r file.zip /kaggle/working/Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading captions\n",
    "import os \n",
    "\n",
    "with open(os.path.join(BASE_DIR,\"captions.txt\"),\"r\") as c:\n",
    "  next(c)\n",
    "  read_captions = c.read() # reading the file 'captions.txt' into read_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating mapping of image to their captions\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def create_mapping(doc):\n",
    "  mapping = {}\n",
    "\n",
    "  for line in tqdm(doc.split(\"\\n\")):\n",
    "    if len(line) < 2:\n",
    "      continue\n",
    "    words = line.split(\",\")\n",
    "    img_name = words[0]\n",
    "    image_id = img_name.split(\".\")[0]\n",
    "\n",
    "    caption = words[1:]\n",
    "    caption = \" \".join(caption)\n",
    "\n",
    "    if image_id not in mapping:\n",
    "      mapping[image_id] = []\n",
    "\n",
    "    mapping[image_id].append(caption)\n",
    "\n",
    "  return mapping\n",
    "\n",
    "mapping = create_mapping(read_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_ids = list(mapping.keys())\n",
    "\n",
    "train, test = train_test_split(image_ids, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(WORKING_DIR,'mapping.pkl'),'wb') as f:\n",
    "    pickle.dump(mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(WORKING_DIR,'mapping.pkl'),'rb') as r:\n",
    "    mapping = pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping['3662909101_21b9e59a3e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(mapping):\n",
    "  for id,captions in mapping.items():\n",
    "    for i in range(len(captions)):\n",
    "      caption = captions[i]\n",
    "      \n",
    "      temp = []\n",
    "      for word in caption.split():\n",
    "        if len(word) > 1:\n",
    "          temp.append(word)\n",
    "      caption = \"<SOS> \" + ' '.join(temp) + \" <EOS>\"\n",
    "      captions[i] = caption\n",
    "\n",
    "preprocess(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping['3662909101_21b9e59a3e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_caption_for_test(image_name):\n",
    "    hidden = decoder.reset_state(batch_size=1) \n",
    "    \n",
    "    img_features_path = WORKING_DIR + '/Features/' + image_name + '.pkl'\n",
    "    \n",
    "    with open(img_features_path, 'rb') as f:\n",
    "        img_tensor_val = pickle.load(f)\n",
    "        \n",
    "    #img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3])) \n",
    "    \n",
    "    features = encoder(img_tensor_val)\n",
    "    \n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<eos>']], 0)\n",
    "    result = []\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<eos>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted = list(), list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    captions = mapping[key]\n",
    "    #print(captions)\n",
    "    \n",
    "    key = key + \".jpg\"\n",
    "    result, _ = evaluate(key)\n",
    "    \n",
    "    for i in result:\n",
    "      if i==\"<unk>\":\n",
    "          result.remove(i)\n",
    "    \n",
    "    actual_captions = []\n",
    "    \n",
    "        \n",
    "    for caption in captions:\n",
    "        cap = list()\n",
    "        words = caption.split()\n",
    "        \n",
    "        for word in words:\n",
    "            if word in ['<SOS>','<EOS>']:\n",
    "                continue\n",
    "            cap.append(word)\n",
    "        actual_captions.append(cap)\n",
    "    \n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(result)\n",
    "    \n",
    "#     print(actual)\n",
    "#     print(predicted)\n",
    "   \n",
    "\n",
    "print(\"Bleu Score for Image Captioning(Inceptionv3)\")\n",
    "\n",
    "smoothing_function = SmoothingFunction()\n",
    "\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0), smoothing_function = smoothing_function.method1))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0), smoothing_function = smoothing_function.method1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORKING_DIR + '/actual.pkl','wb') as a:\n",
    "    pickle.dump(actual, a)\n",
    "    \n",
    "with open(WORKING_DIR + '/predicted.pkl','wb') as p:\n",
    "    pickle.dump(predicted, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bleu Score for Image Captioning(VGG16)\")\n",
    "\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
